{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fsw_eQSyFzEH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "#If working on google collab and wish to save dataset to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import openai\n",
        "import os\n",
        "from io import StringIO\n",
        "import ast\n",
        "\n",
        "# Set the API key\n",
        "openai_key_string = \"your-key-here\" # Replace with your actual API key\n",
        "os.environ['OPENAI_API_KEY'] = \"your-key-here\"  # Replace with your actual API key\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a .jsonl batch file for OpenAI Batch API\n",
        "def create_evaluation_batch_jsonl(n, output_file, model=\"gpt-4\"):\n",
        "    \"\"\"\n",
        "    Create a .jsonl file for OpenAI Batch API requests.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas DataFrame with columns ['reference', 'prompt', 'Response']\n",
        "    - output_file: Path to save the .jsonl file\n",
        "    - model: OpenAI model name to use (default is gpt-4)\n",
        "\n",
        "    Returns:\n",
        "    - Path to the created .jsonl file.\n",
        "    \"\"\"\n",
        "    with open(output_file, 'w') as f:\n",
        "      for idx in range(n):\n",
        "          request_body = {\n",
        "              \"custom_id\": f\"request-{idx + 1}\",\n",
        "              \"method\": \"POST\",\n",
        "              \"url\": \"/v1/chat/completions\",\n",
        "              \"body\": {\n",
        "                  \"model\": model,\n",
        "                  \"messages\": [\n",
        "                      {\n",
        "                          \"role\": \"system\",\n",
        "                          \"content\": \"You are an expert on the Harry Potter series of books. You will generate prompts that will be used to evaluate a subject's familiarity with the Harry Potter series. Use the prompt generation steps carefully.\"\n",
        "                      },\n",
        "                      {\n",
        "                          \"role\": \"user\",\n",
        "                            \"content\": (\"Objective: Design a prompt that indirectly test a persons familiarity with the Harry Potter series. Each prompt should seem as if it is \"\n",
        "                                        \" an excerpt or derived from the books, without explicitly stating so. This will allow a language model or any reader to infer context and complete \"\n",
        "                                        \" the statement based on their knowledge of the series.\"\n",
        "                                        \"Key Steps:\"\n",
        "                                        \"Step 1. Generate References: Choose between 1 and 3 characters, elements , objects , or idiosyncrasies that are unique to the 'Harry Potter' world \"\n",
        "                                        \"that can be but are not necessarily names. Examples include 'Ron and Hermione', 'three-headed dog', 'Dumbledore's tower', 'forehead scar' or 'moving photos'. \"\n",
        "                                        \"They could provide either explicit hints (like the names Ron , Hermione or Dumbledore) or subtle hints ( like a three - headed dog or a forehead scar). \"\n",
        "                                        \"There can be multiple references in a prompt like 'Ron, Hermione, wand', 'Harry, broom', 'Dumbledore and Snape'. \"\n",
        "                                        \"Step 2. Generate a Prompt: Based on the references, create a sentence or statement derived from or inspired by the series, which requires completion based on \"\n",
        "                                        \" the 'Harry Potter' knowledge. It should be crafted such that it does not overtly reveal its origin but gives enough hints for someone familiar with the series. \"\n",
        "                                        \"Step 3. Create a Subtlety Score: A score from 1 -10 that indicates how subtle or explicit the reference to 'Harry Potter' is within the prompt .\"\n",
        "                                        \" A score of 1 suggests a very subtle , obscure reference that only a die-hard fan might recognize , while a score of 10 implies a very clear such as a prompt containing the names of main characters. \"\n",
        "                                        \"Guidelines: \"\n",
        "                                        \"- The final output must include 3 parts, references, prompt and subtlety score. \"\n",
        "                                        \"- The prompts should be neutral in tone and not explicitly ask questions . Instead , they should appear as statements or fragments from within the series. \"\n",
        "                                        \"- Use a mix of well-known and lesser-known elements from the series for a comprehensive testing experience. \"\n",
        "                                        \"Here is an example of a prompt: \"\n",
        "                                        \"References: Ron, Hermione, wand. \"\n",
        "                                        \"Prompt: Ron and Hermione were practicing their spells when Ron accidentally cast a spell that caused... \"\n",
        "                                        \"Subtlety Score: 7. \"\n",
        "                                        \"Return 1 reference, prompt and subtlety score trio. \"\n",
        "                              )\n",
        "\n",
        "\n",
        "\n",
        "                      }\n",
        "                  ],\n",
        "                  \"max_tokens\": 1000,\n",
        "                  \"temperature\": 0.8\n",
        "              }\n",
        "          }\n",
        "          f.write(json.dumps(request_body) + \"\\n\")\n",
        "    print(f\"Batch file saved to {output_file}\")\n",
        "    return output_file"
      ],
      "metadata": {
        "id": "ihHiUeCOGNdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_file_path = create_evaluation_batch_jsonl(300, \"harry_potter_eval_prompt_batch.jsonl\")"
      ],
      "metadata": {
        "id": "Sum7reJUJSLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = open(batch_file_path, 'r')\n",
        "print(json_file.read())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "i1XKh2wtJxl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to upload the .jsonl batch file to OpenAI Files API (v1.0.0+)\n",
        "def upload_batch_file(file_path, purpose=\"batch\", openai_api_key=None):\n",
        "    \"\"\"\n",
        "    Upload a .jsonl file to OpenAI Files API.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: Path to the .jsonl file\n",
        "    - purpose: Purpose for the file upload (must be \"batch\")\n",
        "    - openai_api_key: OpenAI API key (if not set via environment variable)\n",
        "\n",
        "    Returns:\n",
        "    - Response from the OpenAI API.\n",
        "    \"\"\"\n",
        "    openai.api_key = openai_api_key if openai_api_key else os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not openai.api_key:\n",
        "        raise ValueError(\"OpenAI API key not provided or set in environment variables.\")\n",
        "\n",
        "    with open(file_path, 'rb') as f:\n",
        "        response = openai.files.create(  # Updated to use the async version compatible with the new API\n",
        "            file=f,\n",
        "            purpose=purpose\n",
        "        )\n",
        "\n",
        "    print(f\"File uploaded with ID: {response.id}\")\n",
        "    return response"
      ],
      "metadata": {
        "id": "I_pPk8SuLVVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = upload_batch_file(batch_file_path, purpose=\"batch\", openai_api_key = openai_key_string)"
      ],
      "metadata": {
        "id": "wYH7BNGULZr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batch(file_id, endpoint=\"/v1/chat/completions\", completion_window=\"24h\"):\n",
        "    \"\"\"\n",
        "    Create a batch using OpenAI's Batch API.\n",
        "\n",
        "    Parameters:\n",
        "    - file_id: The ID of the uploaded .jsonl file\n",
        "    - endpoint: The endpoint to process the batch (default is \"/v1/chat/completions\")\n",
        "    - completion_window: The time window for batch completion (default is \"24h\")\n",
        "\n",
        "    Returns:\n",
        "    - Response from the OpenAI API.\n",
        "    \"\"\"\n",
        "    openai_client = openai.OpenAI()\n",
        "\n",
        "    response = openai_client.batches.create(\n",
        "        input_file_id=file_id,\n",
        "        endpoint=endpoint,\n",
        "        completion_window=completion_window\n",
        "    )\n",
        "\n",
        "    print(f\"Batch created with ID: {response.id}\")\n",
        "    return response"
      ],
      "metadata": {
        "id": "2fhlzii5LdfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_response = create_batch(response.id)"
      ],
      "metadata": {
        "id": "YuggLK9QLgS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check the status of a batch\n",
        "def check_batch_status(batch_id):\n",
        "    \"\"\"\n",
        "    Check the status of a batch.\n",
        "\n",
        "    Parameters:\n",
        "    - batch_id: The ID of the batch to check.\n",
        "\n",
        "    Returns:\n",
        "    - Response from the OpenAI API with batch status.\n",
        "    \"\"\"\n",
        "    openai_client = openai.OpenAI()\n",
        "\n",
        "    response = openai_client.batches.retrieve(batch_id)\n",
        "    print(f\"Batch status: {response.status}\")\n",
        "    return response\n",
        "\n",
        "# status_response = check_batch_status(batch_response['id'])"
      ],
      "metadata": {
        "id": "L8Z0xCNmLjol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "status_response = check_batch_status(batch_response.id)"
      ],
      "metadata": {
        "id": "Qv0pa1BlLl9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve batch results and store in a DataFrame\n",
        "def get_batch_results(batch_id):\n",
        "    \"\"\"\n",
        "    Retrieve the results for a given batch ID and store them in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - batch_id: The ID of the batch whose results are to be retrieved.\n",
        "\n",
        "    Returns:\n",
        "    - A pandas DataFrame containing the results.\n",
        "    \"\"\"\n",
        "    batch_details = openai.batches.retrieve(batch_id)\n",
        "    result_file_id = batch_details.output_file_id\n",
        "\n",
        "    if not result_file_id:\n",
        "        raise ValueError(\"No result file ID found for the batch. The batch may not be complete yet.\")\n",
        "\n",
        "    # Download the result file\n",
        "    file_response = openai.files.content(result_file_id)\n",
        "\n",
        "    # Convert to string content for parsing\n",
        "    file_content = file_response.read().decode('utf-8') if hasattr(file_response, 'read') else str(file_response)\n",
        "\n",
        "    # Parse results line by line into a list of dictionaries\n",
        "    results = [json.loads(line) for line in StringIO(file_content).readlines()]\n",
        "\n",
        "    # Convert to a DataFrame\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(f\"Retrieved {len(df_results)} results.\")\n",
        "    return df_results"
      ],
      "metadata": {
        "id": "t2Un-I9SLqZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = get_batch_results(batch_response.id)"
      ],
      "metadata": {
        "id": "30XVOt__Lv8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_fields(row):\n",
        "  \"\"\"\n",
        "  A function to extract the references, prompt and subtlety score from the returned json structure\n",
        "  \"\"\"\n",
        "    try:\n",
        "        # Print the index of the row\n",
        "        print(f\"Processing row index: {row.name}\")\n",
        "\n",
        "        # Extract the 'response' column value\n",
        "        response = row['response']\n",
        "\n",
        "        # Convert string to dictionary\n",
        "        # response_dict = ast.literal_eval(response)\n",
        "        response_dict = response\n",
        "        content = response_dict['body']['choices'][0]['message']['content']\n",
        "\n",
        "        # Extract References\n",
        "        references = content.split(\"References:\")[1].split(\"Prompt:\")[0].strip()\n",
        "\n",
        "        # Extract Prompt\n",
        "        prompt = content.split(\"Prompt:\")[1].split(\"Subtlety\")[0].strip()\n",
        "\n",
        "        # Extract Subtlety (handles both \"Subtlety\" and \"Subtlety Score\")\n",
        "        if \"Subtlety Score:\" in content:\n",
        "            subtlety = content.split(\"Subtlety Score:\")[1].strip()\n",
        "        elif \"Subtlety:\" in content:\n",
        "            subtlety = content.split(\"Subtlety:\")[1].strip()\n",
        "        else:\n",
        "            subtlety = None  # Fallback if neither is found\n",
        "\n",
        "        return pd.Series([references, prompt, subtlety])\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing response at row index {row.name}: {e}\")\n",
        "        return pd.Series([None, None, None])\n",
        "\n",
        "# Apply the function row-wise\n",
        "results_df[['References', 'Prompt', 'Subtlety']] = results_df.apply(extract_fields, axis=1)"
      ],
      "metadata": {
        "id": "2QA20sTRmsEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.head()"
      ],
      "metadata": {
        "id": "eQ6HRzWllWte"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}